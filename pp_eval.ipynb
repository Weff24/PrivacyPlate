{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTugjTlcIUU3/VRpxg/nzo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Weff24/PrivacyPlate/blob/main/pp_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!pip install -U yolov5"
      ],
      "metadata": {
        "id": "20KTWyDxc2Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup packages and requirements"
      ],
      "metadata": {
        "id": "Esu8o51-n3Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import yolov5\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = ( r'/usr/bin/tesseract' )"
      ],
      "metadata": {
        "id": "OdVPgHpmc4Vj"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_license_plate_text(image_path):\n",
        "    # Read the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply thresholding or other preprocessing techniques if needed\n",
        "    # Example: gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "    # Use Tesseract OCR to extract text\n",
        "    text = pytesseract.image_to_string(gray, config='--psm 7 --oem 1')\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "image_path = 'test12.png'\n",
        "result = extract_license_plate_text(image_path)\n",
        "print('Extracted License Plate Text:', result)\n"
      ],
      "metadata": {
        "id": "dQMh11Yed8Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Setup methods and functions for image extraction and bounding box recognition"
      ],
      "metadata": {
        "id": "iuuX6KKWoFn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Text Extract (CV2 + Pytesseract)"
      ],
      "metadata": {
        "id": "B3rEJY_3oLnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_license_plate_text(image_path):\n",
        "    # Read the image using OpenCV\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply thresholding or other preprocessing techniques if needed\n",
        "    # Example: gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "    # Use Tesseract OCR to extract text\n",
        "    text = pytesseract.image_to_string(gray, config='--psm 7 --oem 1')\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Rwi8lk4JoTVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YoloV5 model\n",
        "def load_model_bounding():\n",
        "  model = yolov5.load('keremberke/yolov5m-license-plate')\n",
        "\n",
        "  # set model parameters\n",
        "  model.conf = 0.5  # NMS confidence threshold\n",
        "  model.iou = 0.45  # NMS IoU threshold\n",
        "  model.agnostic = False  # NMS class-agnostic\n",
        "  model.multi_label = False  # NMS multiple labels per box\n",
        "  model.max_det = 1000  # maximum number of detections per image\n",
        "\n",
        "  return model\n",
        "\n",
        "def bounding_box_show(image_path, model):\n",
        "  # perform inference\n",
        "  results = model(image_path, size=640)\n",
        "\n",
        "  # inference with test time augmentation\n",
        "  results = model(image_path, augment=True)\n",
        "\n",
        "  # parse results\n",
        "  predictions = results.pred[0]\n",
        "  boxes = predictions[:, :4] # x1, y1, x2, y2\n",
        "  scores = predictions[:, 4]\n",
        "  categories = predictions[:, 5]\n",
        "\n",
        "  # show detection bounding boxes on image\n",
        "  results.show()"
      ],
      "metadata": {
        "id": "XFd0ihltojG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Run Images Throught Pipeline"
      ],
      "metadata": {
        "id": "pTaiLQqJpCGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bounding_box_model = load_model_bounding()\n",
        "\n",
        "image_path_arr = [] # TODO HERE\n",
        "\n",
        "for img_path in image_path_arr:\n",
        "  extracted_text = extract_license_plate_text(img_path)\n",
        "\n",
        "  # 1. Extracted text first\n",
        "  if len(extracted_text) != 0:\n",
        "    print(f\"Extracted text from - {img_path}: {extracted_text}\")\n",
        "    continue\n",
        "\n",
        "  # 2. Run YOLO model to show bounding box if the extracted text is empty\n",
        "  bounding_box_show(img_path, bounding_box_model)"
      ],
      "metadata": {
        "id": "-p3cD04kpFba"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}